{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leitura e processamento dos dados no formato hdf\n",
        "\n",
        "Versão: 0.1 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-9cd9MBU-p3"
      },
      "outputs": [],
      "source": [
        "# ---------------- LEGACY ---------------------------------\n",
        "# code 1: opening as a netCDF4\n",
        "# (example from https://hdfeos.org/zoo/index_openGESDISC_Examples.php)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# import os\n",
        "\n",
        "# # import matplotlib as mpl\n",
        "# # import matplotlib.pyplot as plt\n",
        "# # from mpl_toolkits.basemap import Basemap\n",
        "# import numpy as np\n",
        "# from netCDF4 import Dataset\n",
        "# my_path = '/home/mapbiomasar/Mayra/AIRS/AIRS_L3/CH4_CO_L3_2002_today/'  \n",
        "# my_filename = 'AIRS.2022.02.16.L3.RetStd_IR001.v6.0.32.0.G22048195935.hdf'  \n",
        "# nc = Dataset(my_path + my_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# initialization cell\n",
        "# reads the selected columns in hdf files using the python wrapper (pyhdf)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pyhdf.SD import SD, SDC\n",
        "\n",
        "\n",
        "my_path = '/home/mapbiomasar/Mayra/AIRS/AIRS_L3/CH4_CO_L3_2002_today/'\n",
        "\n",
        "\n",
        "my_filename = 'AIRS.2012.08.30.L3.RetStd_IR001.v6.0.9.0.G14056074243.hdf'\n",
        "hdf = SD(my_path+my_filename, SDC.READ)\n",
        "\n",
        "# # Uncomment to list available SDS datasets\n",
        "# print (hdf.datasets())\n",
        "\n",
        "# # # # # # #  READ VARIABLE FROM HDF FILE  # # # # # # #  \n",
        "def GetHDFData(data1,data2):\n",
        "    \n",
        "    # extract variables of interest\n",
        "    data3D = hdf.select(data1)\n",
        "    data1 = data3D[0,:,:]\n",
        "\n",
        "    data3D = hdf.select(data2)\n",
        "    data2 = data3D[0,:,:]\n",
        "    \n",
        "    # Read geolocation dataset.\n",
        "    lat = hdf.select('Latitude')\n",
        "    latitude = lat[:,:]\n",
        "    lon = hdf.select('Longitude')\n",
        "    longitude = lon[:,:]\n",
        "\n",
        "    # reshape, and transpose values\n",
        "    data = np.array([latitude.reshape(-1),\n",
        "                    longitude.reshape(-1),\n",
        "                    data1.reshape(-1),\n",
        "                    data2.reshape(-1)]).transpose()\n",
        "\n",
        "    # ----- Uncomment this code to change the -9999 to N/A    \n",
        "    \n",
        "    # Handle fill value.\n",
        "    attrs = data3D.attributes(full=1)\n",
        "    fillvalue=attrs[\"_FillValue\"]\n",
        "\n",
        "    # fillvalue[0] is the attribute value.\n",
        "    fv = fillvalue[0]\n",
        "    data[data == fv] = np.nan\n",
        "    data = np.ma.masked_array(data, np.isnan(data))\n",
        "    \n",
        "    # ----- end handle N/A s\n",
        " \n",
        "    return data\n",
        "# # # # # # #  END OF READER  # # # # # # #  \n",
        "\n",
        "# # get data and build dataframe \n",
        "# data1 = 'CO_VMR_A'\n",
        "# data2 = 'CO_VMR_D'\n",
        "# co_meas = GetHDFData(data1,data2)\n",
        "# df  = pd.DataFrame(co_meas, columns = ['lat','lon','co_asc','co_desc'])\n",
        "\n",
        "# # stacking the monthly values\n",
        "# # TODO\n",
        "\n",
        "# # print(co_desc.shape)\n",
        "# # temp  = np.vstack([co_desc,co_asc])\n",
        "# # print(temp.shape)\n",
        "\n",
        "# co_meas.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROCESSING:  2015  files\n",
            "PROCESSING:  2016  files\n"
          ]
        },
        {
          "ename": "HDF4Error",
          "evalue": "SD (7): Error opening file",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHDF4Error\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb#ch0000028?line=23'>24</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlon\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mco_asc\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mco_desc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb#ch0000028?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(path_file):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb#ch0000028?line=26'>27</a>\u001b[0m     \u001b[39m# print(filename)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb#ch0000028?line=27'>28</a>\u001b[0m     hdf \u001b[39m=\u001b[39m SD(filename, SDC\u001b[39m.\u001b[39;49mREAD)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb#ch0000028?line=28'>29</a>\u001b[0m     co_meas \u001b[39m=\u001b[39m GetHDFData(data1,data2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapbiomasar/GitHub/mapbiomas-air/notebooks/hdfReader.ipynb#ch0000028?line=29'>30</a>\u001b[0m     tmp  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([tmp,co_meas])\n",
            "File \u001b[0;32m~/miniconda3/envs/geoproc/lib/python3.9/site-packages/pyhdf/SD.py:1429\u001b[0m, in \u001b[0;36mSD.__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[39mraise\u001b[39;00m HDF4Error(\u001b[39m\"\u001b[39m\u001b[39mSD: bad mode, READ or WRITE must be set\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1428\u001b[0m \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39mSDstart(path, mode)\n\u001b[0;32m-> 1429\u001b[0m _checkErr(\u001b[39m'\u001b[39;49m\u001b[39mSD\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mid\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcannot open \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m path)\n\u001b[1;32m   1430\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id \u001b[39m=\u001b[39m \u001b[39mid\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/geoproc/lib/python3.9/site-packages/pyhdf/error.py:23\u001b[0m, in \u001b[0;36m_checkErr\u001b[0;34m(procName, val, msg)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m : \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (procName, msg)\n\u001b[0;32m---> 23\u001b[0m \u001b[39mraise\u001b[39;00m HDF4Error(err)\n",
            "\u001b[0;31mHDF4Error\u001b[0m: SD (7): Error opening file"
          ]
        }
      ],
      "source": [
        "# this loop will read all the files in a per month basis and aggregate\n",
        "import glob\n",
        "import time\n",
        "yr = list(range(2015,2021))\n",
        "mon = ['01','02','03','04','05','06',\n",
        "       '07','08','09','10','11','12']\n",
        "# mon = ['07','08', '09']\n",
        "# yr = ['2003','2013']\n",
        "data1 = 'CO_VMR_A'\n",
        "data2 = 'CO_VMR_D'\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "# df = pd.DataFrame(columns = ['lat','lon','co_asc','co_desc'])\n",
        "for y in yr:\n",
        "    print('PROCESSING: ', y, ' files')\n",
        "    for m in mon:\n",
        "        path_file = my_path + 'AIRS.' + str(y) + '.'+ m +'.' + '*.hdf'\n",
        "        \n",
        "        # gambi 1: zera o vetor que armazena os valores mensais\n",
        "        # define o novo df que sera exportado\n",
        "        tmp = np.empty([1,4])\n",
        "        tmp[:] = np.nan\n",
        "        df = pd.DataFrame(columns = ['lat','lon','co_asc','co_desc'])\n",
        "        \n",
        "        for filename in glob.glob(path_file):\n",
        "            # print(filename)\n",
        "            try:\n",
        "                hdf = SD(filename, SDC.READ)\n",
        "                co_meas = GetHDFData(data1,data2)\n",
        "                tmp  = np.vstack([tmp,co_meas])\n",
        "            except:\n",
        "                print(\"Could not open \", filename)\n",
        "                break\n",
        "        # build df with the time information\n",
        "        df2 = pd.DataFrame(tmp,columns = ['lat','lon','co_asc','co_desc'])\n",
        "        df2['time'] = str(y) + '-' + m\n",
        "        df = pd.concat([df,df2])\n",
        "        df = df.dropna()\n",
        "            \n",
        "        # process the monthly files and export to CSV\n",
        "        df_co = df.groupby(['time','lat','lon']).mean().reset_index()\n",
        "        df_co.to_csv(my_path + 'CO_' + str(y) + '_' + m + '.csv')\n",
        "        df.co = None\n",
        "print('Processing time:', (time.time()-start_time)/60 , ' min')\n",
        "\n",
        "# df2['time'] = '2022-06'\n",
        "# df2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: verificar o problema que deu no arquivo de 2014\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2199498, 5)\n",
            "(185231, 5)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)\n",
        "print(df2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.08421512545135298"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "185231/2199498"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LydcHU46WpWB",
        "outputId": "e3ba07f0-b8a9-4c54-8fad-60f761cc5f4c"
      },
      "outputs": [],
      "source": [
        "# build geopandas dataframes\n",
        "\n",
        "import geopandas as gpd \n",
        "from shapely.geometry import Polygon, Point, MultiPolygon\n",
        "\n",
        "crs = {'init':'EPSG:4326'}\n",
        "geometry = [Point(xy) for xy in zip(df_asc['lon'], df_asc['lat'])]\n",
        "\n",
        "points = gpd.GeoDataFrame(df_asc, \n",
        "                          crs = crs, \n",
        "                          geometry = geometry)\n",
        "\n",
        "# optional: filter to a square covering South America\n",
        "\n",
        "min_lat = -57.5858\n",
        "max_lat =  15.5988\n",
        "min_lon = -85.8360\n",
        "max_lon = -35.8750\n",
        "\n",
        "# back to original nomenclature\n",
        "tmp = points.copy()\n",
        "\n",
        "lat_filter = (tmp[\"lat\"] >= min_lat) & (tmp[\"lat\"] <= max_lat)\n",
        "lon_filter = (tmp[\"lon\"] >= min_lon) & (tmp[\"lon\"] <= max_lon)\n",
        "\n",
        "# tmp = tmp.loc[lat_filter & lon_filter]\n",
        "\n",
        "points = tmp[lat_filter & lon_filter]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "buffers = points.buffer(0.5) # 1 x 1 degree grid\n",
        "bounds = buffers.bounds\n",
        "\n",
        "bounds['pixel_area'] = bounds.apply(\n",
        "  lambda obj: Polygon(shell=[\n",
        "    Point(obj['maxx'],obj['miny']),\n",
        "    Point(obj['minx'],obj['miny']),\n",
        "    Point(obj['minx'],obj['maxy']),\n",
        "    Point(obj['maxx'],obj['maxy']),\n",
        "    Point(obj['maxx'],obj['miny']),\n",
        "  ]),\n",
        "  axis=1\n",
        ")\n",
        "\n",
        "# adding indexes to merge dataframes\n",
        "points.insert(0, 'New_ID', range(1, 1 + len(points)))\n",
        "bounds.insert(0, 'New_ID', range(1, 1 + len(bounds)))\n",
        "\n",
        "my_merge = bounds.merge(points, on='New_ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ToShapeFile(gdf,yr,mon):\n",
        "  gdf = gpd.GeoDataFrame(\n",
        "    gdf,\n",
        "    crs = {'init':'EPSG:4326'}, \n",
        "    geometry = [a for a in gdf['pixel_area']]                               \n",
        "    )\n",
        "  gdf = gdf.reset_index()\n",
        "  gdf = gdf.loc[:,gdf.columns.isin(['xch4', 'geometry'])]\n",
        "  gdf.to_file('/home/mapbiomasar/MJT/shapefiles/xch4_merged_mips' + str(yr) +'_' + str(mon) + '.shp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui começa o processamento do lote. Primeiramente devo agregar os valores por mes para finalmente montar os shapefiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import time\n",
        "# do not show these annoying warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "df = my_merge.copy()\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['time'])\n",
        "df = df.set_index(df['Date'])\n",
        "df = df.sort_index()\n",
        "df = df[['lat','lon','pixel_area','xch4']]\n",
        "\n",
        "year = list(range(2003,2021))\n",
        "month = list(range(1,13))\n",
        "start_time = time.time()\n",
        "for yr in year:\n",
        "    print('Processing ', yr, ' shapefiles')\n",
        "    for mon in month:\n",
        "        # shapefile_name = '/home/mapbiomasar/MJT/shapefiles/oco2_merged_mips' + str(yr) +'_' + str(mon) +'.shp'\n",
        "        # print(shapefile_name)\n",
        "        # we keep within a month\n",
        "        date_min = datetime.datetime(yr, mon, 1)\n",
        "        date_max = datetime.datetime(yr, mon, 25)\n",
        "        to_process = df[date_min:date_max]\n",
        "        if to_process.empty:        \n",
        "            print('Dataframe from ', str(yr)+str(mon), 'is empty!') \n",
        "        else:\n",
        "            ToShapeFile(to_process,yr,mon)\n",
        "\n",
        "print('Processing time:', time.time()-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_merge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # new geopandas with the variables of interest\n",
        "# gdf = my_merge.copy()\n",
        "# gdf = gpd.GeoDataFrame(\n",
        "#   gdf,\n",
        "#   crs = crs, \n",
        "#   geometry = [a for a in gdf['pixel_area']]                               \n",
        "#   )\n",
        "# gdf = gdf.reset_index()\n",
        "# gdf = gdf.loc[:,gdf.columns.isin(['xco2', 'geometry'])]\n",
        "\n",
        "# # print('--> TYPE: ',type(gdf))\n",
        "# gdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "1mipzd5aWpWC",
        "outputId": "ad4a837c-76ad-4d5e-98cc-b3ca1418975c"
      },
      "outputs": [],
      "source": [
        "# testing the view\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 7))\n",
        "\n",
        "\n",
        "# ax.set_xlim(178.0, 181.0)\n",
        "# ax.set_ylim(20.0, 23.0)\n",
        "world.plot(ax=ax, alpha=0.8, color='grey')\n",
        "\n",
        "gpd.GeoDataFrame(\n",
        "  my_merge,\n",
        "  crs = crs, \n",
        "  geometry = [a for a in my_merge['pixel_area']]                                \n",
        "  ).plot(column='co_asc', ax=ax, legend=True,alpha = 0.4)\n",
        "\n",
        "# gpd.GeoDataFrame(\n",
        "#   points,\n",
        "#   crs = crs, \n",
        "#   geometry = [a for a in points['geometry']]                                \n",
        "#   ).plot(column='xco2', ax=ax)\n",
        "\n",
        "\n",
        "\n",
        "# plt.title('xco2 from Mayra')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "91K4hIs-WpWJ",
        "outputId": "428910fa-1081-4310-818c-0360264aaacf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "  print('--> TYPE: ',type(gdf))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2: reprocessing and splitting files\n",
        "\n",
        "TODO: separar os arquivos por meses para subir ao GEE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxUrgSOXigaA"
      },
      "outputs": [],
      "source": [
        "# # summarizing table\n",
        "# print(len(my_merge))\n",
        "# tmp = my_merge.groupby(['time'])['xco2'].mean()\n",
        "# len(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# looping per year and month\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('--> ',type(gdf))\n",
        "\n",
        "gdf.to_file('/home/mapbiomasar/MJT/notebooks/smallGridMerged.shp')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "oco2-L4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "eef919d14aac40c67e452b43968f34975133eff5504448cca0d6b57e4ab8f96c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
